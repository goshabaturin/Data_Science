{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целях сбора всех известных подключаемых библиотек и импорируемых модулей в одном месте, будем продолжать традицию подключать все что знаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подгрузим pandas\n",
    "import pandas as pd\n",
    "\n",
    "# библиотеку для графиков\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# библиотека для работы с математикой\n",
    "import numpy as np\n",
    "\n",
    "# библиотека для работы со статистикой\n",
    "from scipy import stats as st\n",
    "\n",
    "# метод для разделения выборки на тренировочную и валидационную\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# метод для проведения кросс-валидации\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# метод для создания собственных метрик и вызова их в качестве метрик sklearn\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "#________________________________Классификации____________________________\n",
    "\n",
    "# модель классификации логистической регрессии\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "\n",
    "# # модель классификации дерева решений\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# # модель классификации случайного леса\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# метрики моделей классификации\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# #________________________________Регрессии________________________________\n",
    "\n",
    "# # модель линейной регрессии\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # модель регрессии дерева решений\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# # модель регрессии случайного леса\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # метрики моделей регрессии\n",
    "# from sklearn.metrics import mean_squared_error # mse\n",
    "# from sklearn.metrics import mean_absolute_error # mae\n",
    "# from sklearn.metrics import  r2_score #r^2\n",
    "\n",
    "\n",
    "# #________________________________Перебор параметров модели__________________\n",
    "\n",
    "# # для перебора параметров в моделях случайного леса\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "#________________________________Градиентный бустинг_______________________\n",
    "\n",
    "# градиентный бустинг с кросс-валидацией в библиотеке catboost\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier, Pool, cv\n",
    "\n",
    "# градиентный бустинг в библиотеке LGBM\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "#________________________________Работа с текстом_________________________\n",
    "\n",
    "# библиотека для лемматизации\n",
    "from pymystem3 import Mystem \n",
    "# m = Mystem()  m.lemmatize(text)\n",
    "\n",
    "# библиотека стимменга (для английских текстов)\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "# stemmer = EnglishStemmer(ignore_stopwords=False)\n",
    "# word_list = nltk.word_tokenize(text)\n",
    "# stemmed_output = \" \".join([stemmer.stem(w) for w in word_list])\n",
    "\n",
    "# библиотека для регулярных выражений\n",
    "import re  \n",
    "# re.sub(шаблон, на что заменять, текст, в котором искать совпадения)\n",
    "\n",
    "# библиотека для создания мешка слов\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "# count_vect = CountVectorizer()   \n",
    "# bow = count_vect.fit_transform(corpus)\n",
    "\n",
    "# библиотека для стоп-слов\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('russian')) \n",
    "# count_vect = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# TF-IDF для корпуса текста\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "# count_tf_idf = TfidfVectorizer(stop_words=stopwords) \n",
    "# tf_idf = count_tf_idf.fit_transform(corpus)\n",
    "\n",
    "# для понижение размерности матрицы признаков\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# инициализируем функцию, понижать будем до 100 признаков\n",
    "# svd = TruncatedSVD(n_components=100)\n",
    "# svd.fit(features_train)\n",
    "\n",
    "# для модели BERT\n",
    "import torch\n",
    "import transformers \n",
    "from tqdm import notebook \n",
    "\n",
    "# # код\n",
    "# # токинизируем текст\n",
    "# tokenizer = transformers.BertTokenizer(\n",
    "#     vocab_file='/datasets/ds_bert/vocab.txt')\n",
    "\n",
    "# tokenized = df_tweets['text'].apply(\n",
    "#     lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# max_len = 0\n",
    "# for i in tokenized.values:\n",
    "#     if len(i) > max_len:\n",
    "#         max_len = len(i)\n",
    "\n",
    "# padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "# attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "# config = transformers.BertConfig.from_json_file('/datasets/ds_bert/bert_config.json')\n",
    "# model = transformers.BertModel.from_pretrained('/datasets/ds_bert/rubert_model.bin', config=config)\n",
    "\n",
    "# # будем разбивать на 100 бэтчей для скорости обучения\n",
    "# batch_size = 100\n",
    "# # разобъем на эмбэдинги\n",
    "# embeddings = []\n",
    "# for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "#         batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "#         attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "\n",
    "       \n",
    "#         with torch.no_grad():\n",
    "#             batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "#         embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "# # обучим модель логистической регрессии\n",
    "# features = np.concatenate(embeddings)\n",
    "\n",
    "\n",
    "# # обучите и протестируйте модель\n",
    "# train, test = train_test_split(features, test_size=0.5, random_state=12345)\n",
    "# features_train, features_test, target_train, target_test = train_test_split(features, df_tweets['positive'], test_size=0.5)\n",
    "# model = LogisticRegression()\n",
    "# model.fit(features_train,target_train)\n",
    "# predicted = model.predict(features_train)\n",
    "\n",
    "\n",
    "# #________________________________________________________________________\n",
    "\n",
    "# # стандартизация независимых переменных\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # кодирование категориальных признаков\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# # для перемешивания при up/down-сэмплинге\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# # для более удобного вывода датафреймов, вместо print()\n",
    "# from IPython.display import display\n",
    "\n",
    "# # для разбиения временных рядов на сезонную, трендовую и шумовую состовляющие\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные и посмотрим их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "text     object\n",
       "toxic     int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(159571, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')\n",
    "display(data.head())\n",
    "display(data.isna().sum())\n",
    "display(data.dtypes)\n",
    "display(data.duplicated().sum())\n",
    "display(data['toxic'].unique())\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично. В данные загрузились нормально, в них нет пропусков и дублей, классифицированы они тоже нормально. Всего у нас есть 159 571 классифицированный комментарий.\n",
    "\n",
    "Подготовим эти данные для модели. Поскольку текст английский, то используем стимминг. Общее правило при выборе между лемматизацией и стимменгом: для русского языка- лемматизация, для английского - стимминг. Попутно будем собирать корпус текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = Mystem()\n",
    "\n",
    "\n",
    "stemmer = EnglishStemmer(ignore_stopwords=False)\n",
    "\n",
    "def stem (row):\n",
    "    \n",
    "#     lemm = \" \".join(m.lemmatize(row['text']))    \n",
    "#     return (lemm)\n",
    "    \n",
    "    \n",
    "    word_list = nltk.word_tokenize(row['text'])\n",
    "    \n",
    "    stemmed_output = \" \".join([stemmer.stem(w) for w in word_list])\n",
    "    \n",
    "    \n",
    "    return stemmed_output\n",
    "\n",
    "data['stem_text'] = data.apply(stem, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>stem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explan whi the edit made under my usernam hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww ! he match this background colour i 'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man , i 'm realli not tri to edit war . it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>`` more i ca n't make ani real suggest on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you , sir , are my hero . ani chanc you rememb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                           stem_text  \n",
       "0  explan whi the edit made under my usernam hard...  \n",
       "1  d'aww ! he match this background colour i 'm s...  \n",
       "2  hey man , i 'm realli not tri to edit war . it...  \n",
       "3  `` more i ca n't make ani real suggest on impr...  \n",
       "4  you , sir , are my hero . ani chanc you rememb...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть контакт. Теперь нужно преобразовать текст в вектор. Попробуем рассчитать TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(159571, 160843)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# преобразуем стиммированные тексты в корпус текстов\n",
    "corpus = list(data['stem_text'])\n",
    "\n",
    "# почистим корпус текстов от \"бессмысленных\" слов, например предлогов, союзов и местоимений\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    " \n",
    "# посмотрим, как часто уникальное слово встречается во всём корпусе и в отдельном его тексте\n",
    "# для этого посчитаем tf-idf\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf = count_tf_idf.fit_transform(corpus)\n",
    "\n",
    "display(tf_idf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим features и target для моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 160843)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(159571,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tf_idf_df = pd.DataFrame(tf_idf.toarray(), columns = count_tf_idf.get_feature_names())\n",
    "features = tf_idf\n",
    "target = data['toxic']\n",
    "display(features.shape, target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B.: мы сознательно не преобразовывалы разреженную матрицу в обычный формат, чтоб не умирало ядро. Если нам так надо преобразовать, то данные сначала нужно разбить на train, test, valid, затем обучить векторизацию на train через метод fit_transform(), а данные в test и valid обучать просто через метод transform() иначе, если их обучить через fit_transform(), то получим разные шейпы во всех трех фреймах и нужно будет выбирать \"только те столбцы, что есть во всех трех\". А это тот еще гемморой. А поскольку планируется использовать логистическую регрессию и бустинг через CatBoost, то можно не преобразовыввть - и то и другое умеет работать с разреженной матрицей.\n",
    "\n",
    "\n",
    "И разобъем данные на 2 выборки. И не забудем про стратификацию. Стратификация - это разбиение выборки на тест и трейн таким образом, чтобы и в тесте, и в трейне сохранить баланс классов. Т.е. если у нас есть 900 объектов класса 0 и 100 объектов класса 1 в начальной выборке объёмом 1000, то при использовании X_train, y_train, X_test, y_test = train_test_split(X, y, test_size = 0.1, stratify = y)  в тесте окажется 90 объектов класса 0 и 10 объектов класса 1. В трейне, соответственно, останется 810 и 90 соответственно. Если стратификацию не сделать, то в трейн или в тест в теории вообще могут не попасть объекты класса 1, и модель не научится их классифицировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size = 0.2, stratify = target, random_state =12345)\n",
    "\n",
    "\n",
    "#display(featurest_train.shape, features_test.shape, target_train.shape,  target_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть контакт. Наши данные готовы для обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим логистическую регрессию и посмотрим на ее результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'f1 на test: 0.76'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight='balanced',random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "predict_test = model.predict(features_test)\n",
    "\n",
    "display('f1 на test:{: .2f}'.format(f1_score(predict_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну... достаточно неплохой baseline. По ТЗ нам f1 нужна не меньше 0.75, мы уже выполнили, но попробуем градиентный бустинг. Но для начала нам нужно понизить размерность нашей матрицы features. Поскольку она уже разбита на train, test то понижать будем по отдельности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# инициализируем функцию, понижать будем до 100 признаков\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "\n",
    "# обучим для features_train\n",
    "svd.fit(features_train)\n",
    "\n",
    "# понизим размерность features_train \n",
    "a = svd.transform(features_train)\n",
    "\n",
    "# обучим для features_valid\n",
    "svd.fit(features_test)\n",
    "\n",
    "# понизим размерность features_test\n",
    "b = svd.transform(features_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127656, 100)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(31915, 100)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(a.shape, b.shape, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем CatBoost. С помощью поиска по сетке с кросс-валидацией подберем лучшие параметры модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tloss: 0.5542233\tbest: 0.5542233 (0)\ttotal: 7.48s\tremaining: 1m 7s\n",
      "1:\tloss: 0.5731770\tbest: 0.5731770 (1)\ttotal: 8.35s\tremaining: 33.4s\n",
      "2:\tloss: 0.5974843\tbest: 0.5974843 (2)\ttotal: 10.6s\tremaining: 24.6s\n",
      "3:\tloss: 0.6445360\tbest: 0.6445360 (3)\ttotal: 15.9s\tremaining: 23.9s\n",
      "4:\tloss: 0.5281996\tbest: 0.6445360 (3)\ttotal: 23.3s\tremaining: 23.3s\n",
      "5:\tloss: 0.5705094\tbest: 0.6445360 (3)\ttotal: 45.3s\tremaining: 30.2s\n",
      "6:\tloss: 0.6072240\tbest: 0.6445360 (3)\ttotal: 1m 36s\tremaining: 41.4s\n",
      "7:\tloss: 0.6253976\tbest: 0.6445360 (3)\ttotal: 3m 10s\tremaining: 47.7s\n",
      "8:\tloss: 0.6091596\tbest: 0.6445360 (3)\ttotal: 4m 59s\tremaining: 33.3s\n",
      "9:\tloss: 0.6091596\tbest: 0.6445360 (3)\ttotal: 7m 3s\tremaining: 0us\n",
      "Estimating final quality...\n"
     ]
    }
   ],
   "source": [
    "cv_dataset = Pool(data=a,\n",
    "                  label=target_train,\n",
    "                  cat_features=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_catboost = CatBoostClassifier(loss_function='Logloss', eval_metric='F1')\n",
    "\n",
    "grid = {'max_depth':range(1,16,4),\n",
    "        'learning_rate':[0.5, 1],\n",
    "        'iterations':range(1,20,2)\n",
    "       }\n",
    "\n",
    "random_search_result = model_catboost.randomized_search(grid, \n",
    "                                       X=cv_dataset,\n",
    "                                       cv=3,\n",
    "                                       plot=False)\n",
    "\n",
    "depth_catboost = random_search_result['params']['depth']\n",
    "iterations_catboost = random_search_result['params']['iterations']\n",
    "learning_rate_catboost = random_search_result['params']['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(depth_catboost,iterations_catboost,learning_rate_catboost)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, получили первое приближение, но качество не фонтан, попробуем улучшить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tloss: 0.6445360\tbest: 0.6445360 (0)\ttotal: 10.5s\tremaining: 1m 34s\n",
      "1:\tloss: 0.6371287\tbest: 0.6445360 (0)\ttotal: 15.6s\tremaining: 1m 2s\n",
      "2:\tloss: 0.6426100\tbest: 0.6445360 (0)\ttotal: 21.4s\tremaining: 50s\n",
      "3:\tloss: 0.6446480\tbest: 0.6446480 (3)\ttotal: 26.9s\tremaining: 40.4s\n",
      "4:\tloss: 0.6431937\tbest: 0.6446480 (3)\ttotal: 33.1s\tremaining: 33.1s\n",
      "5:\tloss: 0.6547906\tbest: 0.6547906 (5)\ttotal: 39.7s\tremaining: 26.5s\n",
      "6:\tloss: 0.6517664\tbest: 0.6547906 (5)\ttotal: 47.3s\tremaining: 20.3s\n",
      "7:\tloss: 0.6497808\tbest: 0.6547906 (5)\ttotal: 55.4s\tremaining: 13.9s\n",
      "8:\tloss: 0.6532663\tbest: 0.6547906 (5)\ttotal: 1m 3s\tremaining: 7.06s\n",
      "9:\tloss: 0.6521008\tbest: 0.6547906 (5)\ttotal: 1m 14s\tremaining: 0us\n",
      "Estimating final quality...\n"
     ]
    }
   ],
   "source": [
    "cv_dataset = Pool(data=a,\n",
    "                  label=target_train,\n",
    "                  cat_features=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_catboost = CatBoostClassifier(loss_function='Logloss', eval_metric='F1')\n",
    "\n",
    "grid = {'max_depth':range(5,9,1),\n",
    "        'learning_rate':[0.7,0.9, 1],\n",
    "        'iterations':range(17,19,1)\n",
    "       }\n",
    "\n",
    "random_search_result = model_catboost.randomized_search(grid, \n",
    "                                       X=cv_dataset,\n",
    "                                       cv=3,\n",
    "                                       plot=False)\n",
    "\n",
    "depth_catboost = random_search_result['params']['depth']\n",
    "iterations_catboost = random_search_result['params']['iterations']\n",
    "learning_rate_catboost = random_search_result['params']['learning_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшить показатели конечно получилось, но не особо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4908342\ttotal: 380ms\tremaining: 6.45s\n",
      "1:\tlearn: 0.5623938\ttotal: 779ms\tremaining: 6.23s\n",
      "2:\tlearn: 0.5838892\ttotal: 1.18s\tremaining: 5.9s\n",
      "3:\tlearn: 0.5930233\ttotal: 1.69s\tremaining: 5.9s\n",
      "4:\tlearn: 0.6137110\ttotal: 1.99s\tremaining: 5.17s\n",
      "5:\tlearn: 0.6211458\ttotal: 2.47s\tremaining: 4.94s\n",
      "6:\tlearn: 0.6314673\ttotal: 2.87s\tremaining: 4.52s\n",
      "7:\tlearn: 0.6366980\ttotal: 3.27s\tremaining: 4.09s\n",
      "8:\tlearn: 0.6354988\ttotal: 3.68s\tremaining: 3.68s\n",
      "9:\tlearn: 0.6425499\ttotal: 4.17s\tremaining: 3.33s\n",
      "10:\tlearn: 0.6458234\ttotal: 4.57s\tremaining: 2.91s\n",
      "11:\tlearn: 0.6477967\ttotal: 4.97s\tremaining: 2.48s\n",
      "12:\tlearn: 0.6508410\ttotal: 5.37s\tremaining: 2.07s\n",
      "13:\tlearn: 0.6506392\ttotal: 5.77s\tremaining: 1.65s\n",
      "14:\tlearn: 0.6512313\ttotal: 6.28s\tremaining: 1.25s\n",
      "15:\tlearn: 0.6577707\ttotal: 6.67s\tremaining: 834ms\n",
      "16:\tlearn: 0.6592931\ttotal: 7.16s\tremaining: 421ms\n",
      "17:\tlearn: 0.6623809\ttotal: 7.56s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'f1 на test:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.583880037488285"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CatBoostClassifier(depth = depth_catboost,\n",
    "                          iterations = iterations_catboost,\n",
    "                          learning_rate = learning_rate_catboost,\n",
    "                          loss_function='Logloss', \n",
    "                          eval_metric='F1'\n",
    "                          )\n",
    "\n",
    "model.fit(a, target_train)\n",
    "\n",
    "predict_test = model.predict(b)\n",
    "\n",
    "display('f1 на test:',f1_score(predict_test, target_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 далек от целевых. Значит логистическая регрессия победила. На ней мы получили целевые показатели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF в связке с логистической регрессией и стратификацией дает результаты лучше, чем бустинг и \"стоит\" дешевле, потому как нет необходимости понижать размерность матрицы признаков. Кроме того, он еще и работает шустрее. \n",
    "\n",
    "Векторизацию текста имеет смылс делать до того, как происходит разбиение на выборки. В противном случае, векторизацию нужно обучать методом fit_transform() на тренировочной выборке, затем преобразовывать тестовую и валидационную методом transform() и, поскольку шейпы у всех трех будут разные - приводить их все к одной размерности (либо оставлять общие столбцы, либо дописывать несуществующие столбцы, которые заполнять нулями)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
